name: SERM Pipeline Test Suite
'on':
  push:
    branches:
    - main
    - develop
    - feature/*
  pull_request:
    branches:
    - main
    - develop
  schedule:
  - cron: 0 2 * * *
env:
  POSTGRES_PASSWORD: test_password
  POSTGRES_USER: test_user
  POSTGRES_DB: sydney_ci_test
jobs:
  test-matrix:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        python-version:
        - '3.9'
        - '3.10'
        - '3.11'
        postgres-version:
        - '13'
        - '14'
        - '15'
        os:
        - ubuntu-22.04
        - ubuntu-20.04
    services:
      postgres:
        image: postgres:${{ matrix.postgres-version }}
        env:
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s
          --health-retries 5
        ports:
        - 5432:5432
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: ${{ runner.os }}-pip-
    - name: Install dependencies
      run: 'pip install --upgrade pip

        pip install pytest pytest-cov pytest-asyncio pytest-xdist

        pip install psycopg2-binary asyncio psutil

        pip install numpy pandas matplotlib

        pip install -r requirements.txt || echo "No requirements.txt found"'
    - name: Wait for PostgreSQL
      run: timeout 60s bash -c 'until pg_isready -h localhost -p 5432 -U ${{ env.POSTGRES_USER
        }}; do sleep 2; done'
    - name: Setup test database
      run: "export PGPASSWORD=$POSTGRES_PASSWORD\ncreatedb -h localhost -U $POSTGRES_USER\
        \ sydney_test || echo \"Database already exists\"\ncreatedb -h localhost -U\
        \ $POSTGRES_USER sydney_unit_test || echo \"Database already exists\"  \n\
        createdb -h localhost -U $POSTGRES_USER sydney_e2e_test || echo \"Database\
        \ already exists\"\ncreatedb -h localhost -U $POSTGRES_USER sydney_failure_test\
        \ || echo \"Database already exists\"\ncreatedb -h localhost -U $POSTGRES_USER\
        \ sydney_perf_test || echo \"Database already exists\""
    - name: Run unit tests
      run: pytest sydney/test_serm_units.py -v --junitxml=test-results/unit-tests.xml
        --cov=sydney --cov-report=xml:coverage/unit-coverage.xml
    - name: Run integration tests
      run: pytest sydney/test_serm_integration.py -v --junitxml=test-results/integration-tests.xml
        --cov=sydney --cov-report=xml:coverage/integration-coverage.xml
    - name: Run end-to-end tests
      run: pytest sydney/test_serm_end_to_end.py -v --junitxml=test-results/e2e-tests.xml
        --maxfail=3
      timeout-minutes: 30
    - name: Run failure recovery tests
      run: pytest sydney/test_serm_failure_recovery.py -v --junitxml=test-results/failure-recovery-tests.xml
        --maxfail=5
      timeout-minutes: 20
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}-${{ matrix.postgres-version
          }}-${{ matrix.os }}
        path: test-results/
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage/unit-coverage.xml,./coverage/integration-coverage.xml
        flags: unittests,integration
        name: codecov-${{ matrix.python-version }}-${{ matrix.postgres-version }}
  performance-tests:
    runs-on: ubuntu-22.04
    needs:
    - test-matrix
    if: github.ref == 'refs/heads/main' || github.event_name == 'schedule'
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s
          --health-retries 5
        ports:
        - 5432:5432
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    - name: Install dependencies
      run: 'pip install --upgrade pip

        pip install pytest pytest-cov pytest-asyncio pytest-xdist

        pip install psycopg2-binary asyncio psutil

        pip install numpy pandas matplotlib

        pip install -r requirements.txt || echo "No requirements.txt found"'
    - name: Setup performance test database
      run: "export PGPASSWORD=$POSTGRES_PASSWORD\ncreatedb -h localhost -U $POSTGRES_USER\
        \ sydney_test || echo \"Database already exists\"\ncreatedb -h localhost -U\
        \ $POSTGRES_USER sydney_unit_test || echo \"Database already exists\"  \n\
        createdb -h localhost -U $POSTGRES_USER sydney_e2e_test || echo \"Database\
        \ already exists\"\ncreatedb -h localhost -U $POSTGRES_USER sydney_failure_test\
        \ || echo \"Database already exists\"\ncreatedb -h localhost -U $POSTGRES_USER\
        \ sydney_perf_test || echo \"Database already exists\""
    - name: Run performance benchmarks
      run: pytest sydney/test_serm_performance.py -v --junitxml=test-results/performance-tests.xml
      timeout-minutes: 60
    - name: Analyze performance results
      run: "python -c \"\nimport json\nimport glob\nfrom datetime import datetime\n\
        \n# Analyze benchmark results\nbenchmark_files = glob.glob('sydney/benchmark_*.json')\n\
        performance_summary = {\n    'timestamp': datetime.now().isoformat(),\n  \
        \  'benchmark_count': len(benchmark_files),\n    'files_analyzed': benchmark_files\n\
        }\n\nif benchmark_files:\n    # Load and analyze latest benchmark\n    with\
        \ open(benchmark_files[-1], 'r') as f:\n        data = json.load(f)\n    \
        \    performance_summary['latest_benchmark'] = data.get('summary', {})\n\n\
        # Save performance summary\nwith open('performance_summary.json', 'w') as\
        \ f:\n    json.dump(performance_summary, f, indent=2)\n    \nprint(f'Performance\
        \ analysis complete: {len(benchmark_files)} benchmark files processed')\n\""
    - name: Upload performance artifacts
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: 'sydney/benchmark_*.json

          sydney/memory_*.json

          sydney/db_performance_*.json'
  quality-gates:
    runs-on: ubuntu-22.04
    needs:
    - test-matrix
    - performance-tests
    steps:
    - name: Download test artifacts
      uses: actions/download-artifact@v3
    - name: Analyze test results
      run: "python -c \"\nimport json\nimport xml.etree.ElementTree as ET\nfrom pathlib\
        \ import Path\n\n# Analyze test results\ntest_results = {}\nfor xml_file in\
        \ Path('.').glob('**/test-results/*.xml'):\n    try:\n        tree = ET.parse(xml_file)\n\
        \        root = tree.getroot()\n        \n        test_results[xml_file.name]\
        \ = {\n            'tests': int(root.get('tests', 0)),\n            'failures':\
        \ int(root.get('failures', 0)),\n            'errors': int(root.get('errors',\
        \ 0)),\n            'time': float(root.get('time', 0))\n        }\n    except\
        \ Exception as e:\n        print(f'Error parsing {xml_file}: {e}')\n\n# Calculate\
        \ overall metrics\ntotal_tests = sum(r['tests'] for r in test_results.values())\n\
        total_failures = sum(r['failures'] for r in test_results.values())\ntotal_errors\
        \ = sum(r['errors'] for r in test_results.values())\nsuccess_rate = (total_tests\
        \ - total_failures - total_errors) / total_tests if total_tests > 0 else 0\n\
        \nquality_metrics = {\n    'total_tests': total_tests,\n    'success_rate':\
        \ success_rate,\n    'failure_rate': (total_failures + total_errors) / total_tests\
        \ if total_tests > 0 else 0,\n    'test_results': test_results\n}\n\nwith\
        \ open('quality_metrics.json', 'w') as f:\n    json.dump(quality_metrics,\
        \ f, indent=2)\n    \nprint(f'Quality analysis: {total_tests} tests, {success_rate:.1%}\
        \ success rate')\n\""
    - name: Check quality gates
      run: python sydney/ci_quality_gates.py
