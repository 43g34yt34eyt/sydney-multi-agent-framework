pipeline {
    agent any
    
    environment {
        POSTGRES_PASSWORD = 'test_password'
        POSTGRES_USER = 'test_user'
        POSTGRES_DB = 'sydney_ci_test'
        PYTHONPATH = "${env.WORKSPACE}/sydney"
    }
    
    triggers {
        cron('H 2 * * *')  // Daily at 2 AM
        pollSCM('H/5 * * * *')  // Poll every 5 minutes
    }
    
    options {
        timeout(time: 2, unit: 'HOURS')
        retry(3)
        skipDefaultCheckout()
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
                script {
                    env.BUILD_TIMESTAMP = sh(returnStdout: true, script: 'date +%Y%m%d_%H%M%S').trim()
                }
            }
        }
        
        stage('Setup Environment') {
            parallel {
                stage('Setup Python') {
                    steps {
                        sh 'python3 -m venv venv'
                        sh '. venv/bin/activate && pip install --upgrade pip'
                        sh '. venv/bin/activate && pip install --upgrade pip
pip install pytest pytest-cov pytest-asyncio pytest-xdist
pip install psycopg2-binary asyncio psutil
pip install numpy pandas matplotlib
pip install -r requirements.txt || echo "No requirements.txt found"'
                    }
                }
                
                stage('Setup Database') {
                    steps {
                        sh 'docker run --name postgres-test -e POSTGRES_PASSWORD=$POSTGRES_PASSWORD -e POSTGRES_USER=$POSTGRES_USER -e POSTGRES_DB=$POSTGRES_DB -p 5432:5432 -d postgres:15'
                        sh 'sleep 10'  // Wait for PostgreSQL to start
                        sh 'export PGPASSWORD=$POSTGRES_PASSWORD
createdb -h localhost -U $POSTGRES_USER sydney_test || echo "Database already exists"
createdb -h localhost -U $POSTGRES_USER sydney_unit_test || echo "Database already exists"  
createdb -h localhost -U $POSTGRES_USER sydney_e2e_test || echo "Database already exists"
createdb -h localhost -U $POSTGRES_USER sydney_failure_test || echo "Database already exists"
createdb -h localhost -U $POSTGRES_USER sydney_perf_test || echo "Database already exists"'
                    }
                    post {
                        always {
                            sh 'docker stop postgres-test || true'
                            sh 'docker rm postgres-test || true'
                        }
                    }
                }
            }
        }
        
        stage('Test Execution') {
            parallel {
                stage('Unit Tests') {
                    steps {
                        sh '. venv/bin/activate && pytest sydney/test_serm_units.py -v --junitxml=test-results/unit-tests.xml --cov=sydney --cov-report=xml:coverage/unit-coverage.xml'
                    }
                    post {
                        always {
                            publishTestResults testResultsPattern: 'test-results/unit-tests.xml'
                        }
                    }
                }
                
                stage('Integration Tests') {
                    steps {
                        sh '. venv/bin/activate && pytest sydney/test_serm_integration.py -v --junitxml=test-results/integration-tests.xml --cov=sydney --cov-report=xml:coverage/integration-coverage.xml'
                    }
                    post {
                        always {
                            publishTestResults testResultsPattern: 'test-results/integration-tests.xml'
                        }
                    }
                }
                
                stage('End-to-End Tests') {
                    steps {
                        timeout(time: 30, unit: 'MINUTES') {
                            sh '. venv/bin/activate && pytest sydney/test_serm_end_to_end.py -v --junitxml=test-results/e2e-tests.xml --maxfail=3'
                        }
                    }
                    post {
                        always {
                            publishTestResults testResultsPattern: 'test-results/e2e-tests.xml'
                        }
                    }
                }
            }
        }
        
        stage('Performance Tests') {
            when {
                anyOf {
                    branch 'main'
                    triggeredBy 'TimerTrigger'
                }
            }
            steps {
                timeout(time: 60, unit: 'MINUTES') {
                    sh '. venv/bin/activate && pytest sydney/test_serm_performance.py -v --junitxml=test-results/performance-tests.xml'
                }
                sh 'python -c "
import json
import glob
from datetime import datetime

# Analyze benchmark results
benchmark_files = glob.glob('sydney/benchmark_*.json')
performance_summary = {
    'timestamp': datetime.now().isoformat(),
    'benchmark_count': len(benchmark_files),
    'files_analyzed': benchmark_files
}

if benchmark_files:
    # Load and analyze latest benchmark
    with open(benchmark_files[-1], 'r') as f:
        data = json.load(f)
        performance_summary['latest_benchmark'] = data.get('summary', {})

# Save performance summary
with open('performance_summary.json', 'w') as f:
    json.dump(performance_summary, f, indent=2)
    
print(f'Performance analysis complete: {len(benchmark_files)} benchmark files processed')
"'
            }
            post {
                always {
                    archiveArtifacts artifacts: 'sydney/benchmark_*.json,sydney/memory_*.json,performance_summary.json'
                }
            }
        }
        
        stage('Quality Gates') {
            steps {
                sh 'python -c "
import json
import xml.etree.ElementTree as ET
from pathlib import Path

# Analyze test results
test_results = {}
for xml_file in Path('.').glob('**/test-results/*.xml'):
    try:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        
        test_results[xml_file.name] = {
            'tests': int(root.get('tests', 0)),
            'failures': int(root.get('failures', 0)),
            'errors': int(root.get('errors', 0)),
            'time': float(root.get('time', 0))
        }
    except Exception as e:
        print(f'Error parsing {xml_file}: {e}')

# Calculate overall metrics
total_tests = sum(r['tests'] for r in test_results.values())
total_failures = sum(r['failures'] for r in test_results.values())
total_errors = sum(r['errors'] for r in test_results.values())
success_rate = (total_tests - total_failures - total_errors) / total_tests if total_tests > 0 else 0

quality_metrics = {
    'total_tests': total_tests,
    'success_rate': success_rate,
    'failure_rate': (total_failures + total_errors) / total_tests if total_tests > 0 else 0,
    'test_results': test_results
}

with open('quality_metrics.json', 'w') as f:
    json.dump(quality_metrics, f, indent=2)
    
print(f'Quality analysis: {total_tests} tests, {success_rate:.1%} success rate')
"'
                script {
                    def qualityData = readJSON file: 'quality_metrics.json'
                    
                    if (qualityData.success_rate < 0.8) {
                        error "Quality gate failed: Success rate ${qualityData.success_rate} below threshold 0.8"
                    }
                    
                    echo "Quality gates passed: ${qualityData.success_rate * 100}% success rate"
                }
            }
        }
    }
    
    post {
        always {
            publishTestResults testResultsPattern: 'test-results/*.xml'
            publishCoverage adapters: [coberturaAdapter('coverage/*.xml')], sourceFileResolver: sourceFiles('STORE_LAST_BUILD')
            archiveArtifacts artifacts: 'test-results/**,coverage/**,quality_metrics.json', allowEmptyArchive: true
        }
        
        success {
            echo 'Pipeline completed successfully!'
            slackSend channel: '#serm-pipeline', 
                     color: 'good', 
                     message: "✅ SERM Pipeline tests passed - Build ${env.BUILD_NUMBER}"
        }
        
        failure {
            echo 'Pipeline failed!'
            slackSend channel: '#serm-pipeline', 
                     color: 'danger', 
                     message: "❌ SERM Pipeline tests failed - Build ${env.BUILD_NUMBER}"
        }
        
        unstable {
            echo 'Pipeline is unstable!'
            slackSend channel: '#serm-pipeline', 
                     color: 'warning', 
                     message: "⚠️ SERM Pipeline tests unstable - Build ${env.BUILD_NUMBER}"
        }
    }
}